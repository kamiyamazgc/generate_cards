#!/usr/bin/env python3
"""
Create YAML-Front-Matter + Markdown "information cards" from a list of URLs.
Cards are saved to ./Library/{YYYY-MM-DD-short-slug}.md
"""

import os, sys, json, datetime, pathlib, argparse, getpass
from urllib.parse import urlparse

import httpx
import trafilatura
import re
from langdetect import detect, LangDetectException
import yaml
from slugify import slugify
import dateparser
import openai
from tqdm import tqdm
from dateutil import parser as dtparser


# ensure OpenAI key is configured later via CLI/env/prompt

# ---------- config ---------------------------------------------------------

LIBRARY_DIR = pathlib.Path("Library")
MODEL_NAME  = "gpt-4o-mini"
SUMMARY_TOK = 1000        # rough budget: adjust as you like
TRANS_TOK   = 2048

LIBRARY_DIR.mkdir(exist_ok=True)

# --------- digest file constant ---------
DIGEST_FILE = LIBRARY_DIR / "_daily_digest.md"

with open("ndc10_3rd.json", encoding="utf-8") as f:
    _raw = json.load(f)

# Normalise so every entry is {"ja": "...", "en": "..."} (en may be "")
NDC_LABELS: dict[str, dict[str, str]] = {}
for code, val in _raw.items():
    if isinstance(val, str):
        # only Japanese provided
        NDC_LABELS[code] = {"ja": val, "en": ""}
    elif isinstance(val, dict):
        NDC_LABELS[code] = {"ja": val.get("ja", ""), "en": val.get("en", "")}

# ---------- helpers --------------------------------------------------------

def fetch_html(url: str) -> str:
    """Return HTML as Unicode, trying trafilatura's fetch (robust charset) first."""
    html = trafilatura.fetch_url(url)
    if html:                     # success
        return html
    # fallback
    r = httpx.get(url, follow_redirects=True, timeout=30)
    r.raise_for_status()
    return r.text

def extract_meta(url: str, html: str) -> dict:
    """Return dict with title, date, author, text, keywords."""
    data_json = trafilatura.extract(html, url=url,
                                    output_format="json",
                                    with_metadata=True)
    if not data_json:
        return {}
    d = json.loads(data_json)

    # normalise
    pub_dt = dateparser.parse(d.get("date") or "")  # None if absent
    author = d.get("author") or ""
    fam, given = (author.split(maxsplit=1) + [""])[:2] if author else ("", "")
    keywords = [k.strip() for k in (d.get("keywords") or "").split(",") if k.strip()]

    return {
        "title": d.get("title") or "Untitled",
        "publication_date": pub_dt.date().isoformat() if pub_dt else "",
        "author_family": fam,
        "author_given": given,
        "keywords": keywords,
        "text": d.get("text") or "",
    }

# ---------- language & chunk helpers -------------------------------------

def detect_lang(text: str) -> str:
    try:
        return detect(text)
    except LangDetectException:
        return "unknown"

def chunk_text(text: str, max_chars: int = 4000):
    """Greedy split on sentence boundaries so each chunk fits within token limits."""
    sentences = re.split(r'(?<=[„ÄÇ.!?ÔºÅÔºü])\s*', text)
    chunks, buf = [], ""
    for s in sentences:
        if len(buf) + len(s) > max_chars and buf:
            chunks.append(buf)
            buf = s
        else:
            buf += s
    if buf:
        chunks.append(buf)
    return chunks

def translate_full(text: str) -> str:
    """Translate arbitrarily long texts to Japanese by chunking."""
    translated = []
    for chunk in chunk_text(text):
        translated.append(
            ask_openai(
                "Ê¨°„ÅÆÊñáÁ´†„ÇíÊó•Êú¨Ë™û„Å´Ê≠£Á¢∫„Å´ÂÖ®ÊñáÁøªË®≥„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n" + chunk,
                TRANS_TOK
            )
        )
    return "\n\n".join(translated)


# ---------- markdown tidy helpers ---------------------------------------

_SENT_END_RE = re.compile(r"([„ÄÇÔºé.!?ÔºÅÔºü])\s*\n")

def tidy_markdown_para(text: str) -> str:
    """
    Ensure a blank line between paragraphs for better MD viewers.
    1) Insert an extra newline after sentence‚Äëending punctuation that
       currently has only a single line break.
    2) Collapse 3+ consecutive blank lines to max 2.
    Works for Japanese '„ÄÇ', Chinese 'Ôºé', and Western punctuation.
    """
    txt = _SENT_END_RE.sub(r"\1\n\n", text)
    txt = re.sub(r"\n{3,}", "\n\n", txt)
    return txt.strip()


# ---------- keyword extraction via LLM -----------------------------------

KEYWORD_TOP_N = 8

def extract_keywords_llm(summary: str, top_n: int = KEYWORD_TOP_N):
    """
    Ask GPT‚Äë4o‚Äëmini to return top N Japanese keywords, comma‚Äëseparated.
    """
    prompt = (
        f"‰ª•‰∏ã„ÅÆÊñáÁ´†„ÅÆ‰∏ªË¶Å„Å™„Ç≠„Éº„ÉØ„Éº„Éâ„Çí{top_n}Ë™ûÊäΩÂá∫„Åó„ÄÅ"
        "Êó•Êú¨Ë™û„Ç´„É≥„ÉûÂå∫Âàá„Çä„ÅßËøîÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n"
        f"{summary}"
    )
    resp = ask_openai(prompt, max_tokens=128)
    return [kw.strip() for kw in resp.split(",") if kw.strip()]

def ask_openai(prompt: str, max_tokens: int, model: str | None = None) -> str:
    rsp = openai.chat.completions.create(
        model=model or MODEL_NAME,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=max_tokens,
        temperature=0.3,
    )
    return rsp.choices[0].message.content.strip()

# ---------- NDC classification via LLM -----------------------------------

# concise guideline for the LLM so it focuses on true subject classification
NDC_GUIDELINE = (
    "NDC10 ÂàÜÈ°û„ÇíÊ±∫„ÇÅ„ÇãÈöõ„ÅØ„ÄÅË®ò‰∫ã„ÅÆ„Äé‰∏ªÈ°å„Äè„ÇíÊúÄÂÑ™ÂÖà„Å´Âà§Êñ≠„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n"
    "000 Á∑èË®ò„ÅØ„ÄéÁôæÁßë‰∫ãÂÖ∏„ÉªÂõ≥Êõ∏È§®„ÉªÂá∫Áâà„Éª„Ç∏„É£„Éº„Éä„É™„Ç∫„É†‰∏ÄËà¨„Äè„Å™„Å© "
    "„É°„ÇøÊÉÖÂ†±„Å´ÈôêÂÆö„Åï„Çå„ÇãÂ†¥Âêà„ÅÆ„ÅøÈÅ∏Êäû„Åó„Åæ„Åô„ÄÇ"
    "\n‰∏ª„Å™Âà§Êñ≠Âü∫Ê∫ñ:\n"
    "  ‚Ä¢ ÂÆóÊïô„ÉªÊïô‰ºö ‚Üí 180 Á≥ª\n"
    "  ‚Ä¢ ÂåªÂ≠¶„Éª‰øùÂÅ• ‚Üí 490 Á≥ª\n"
    "  ‚Ä¢ Ëá™ÁÑ∂ÁßëÂ≠¶(Áâ©ÁêÜ/ÂåñÂ≠¶/ÁîüÁâ©) ‚Üí 400 Á≥ª\n"
    "  ‚Ä¢ ÊÉÖÂ†±ÊäÄË°ì„ÉªAI„Éª„Ç≥„É≥„Éî„É•„Éº„Çø ‚Üí 007, 548, 549\n"
    "  ‚Ä¢ ÈÄö‰ø°„Éª„Çπ„Éû„Éº„Éà„Éï„Ç©„É≥ ‚Üí 547\n"
    "  ‚Ä¢ Á§æ‰ºö„ÉªÊîøÊ≤ª ‚Üí 300‚Äë319\n"
    "  ‚Ä¢ ÁµåÊ∏à„Éª‰ºÅÊ•≠„Éª„Éû„Éº„Ç± ‚Üí 330‚Äë338\n"
    "  ‚Ä¢ ÊäÄË°ì„ÉªÂ∑•Â≠¶‰∏ÄËà¨ ‚Üí 500 Á≥ª\n"
    "  ‚Ä¢ Ëä∏Ë°ì„ÉªÈü≥Ê•Ω ‚Üí 700‚Äë769\n"
    "‰∏ÄË¶ß„Å´ÁÑ°„ÅÑ„ÅãËø∑„ÅÜÂ†¥Âêà„ÅØÁ©∫Ê¨Ñ„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
)

NDC_MODEL_NAME = "gpt-4o-mini"

_NDC_CODES_CSV = ", ".join(sorted(NDC_LABELS.keys()))

def classify_ndc_llm(title: str, summary: str) -> str:
    """
    Ask GPT model for the best 3‚Äëdigit NDC10 code.
    1) First prompt: free answer
    2) If result is invalid (not in list), re‚Äëprompt with explicit choices
    Returns "" when still invalid.
    """
    def _ask(prompt: str) -> str:
        resp = ask_openai(prompt, max_tokens=10, model=NDC_MODEL_NAME)
        code = resp.strip()[:3]
        return code if code.isdigit() and code in NDC_LABELS else ""

    # first attempt ‚Äì free form
    base_prompt = (
        f"{NDC_GUIDELINE}\n\n"
        "Ê¨°„ÅÆ„Çø„Ç§„Éà„É´„Å®Ë¶ÅÁ¥Ñ„Å´ÊúÄ„ÇÇÈÅ©Âàá„Å™Êó•Êú¨ÂçÅÈÄ≤ÂàÜÈ°ûÊ≥ï(NDC10)„ÅÆ3Ê°ÅÂàÜÈ°û„Ç≥„Éº„Éâ„Çí"
        "‰∏Ä„Å§„Å†„ÅëÂçäËßíÊï∞Â≠ó„ÅßÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÂ≠òÂú®„Åó„Å™„ÅÑ„Ç≥„Éº„Éâ„ÇÑ4Ê°Å‰ª•‰∏ä„ÅØ‰∏çÂèØ„ÄÇ"
        "Ëø∑„ÅÜÂ†¥Âêà„ÅØÁ©∫Ê¨Ñ„ÅßËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n"
        f"„Çø„Ç§„Éà„É´: {title}\nË¶ÅÁ¥Ñ: {summary}"
    )
    code = _ask(base_prompt)
    if code:
        return code  # valid within list

    # second attempt ‚Äì force choice from list
    choice_prompt = (
        f"{NDC_GUIDELINE}\n\n"
        "‰ª•‰∏ã„ÅØ NDC10 „ÅÆÊúâÂäπ„Å™3Ê°ÅÂàÜÈ°û„Ç≥„Éº„Éâ‰∏ÄË¶ß„Åß„Åô„ÄÇ\n"
        f"{_NDC_CODES_CSV}\n\n"
        "Ê¨°„ÅÆ„Çø„Ç§„Éà„É´„Å®Ë¶ÅÁ¥Ñ„Å´ÊúÄ„ÇÇÈÅ©Âàá„Å™„Ç≥„Éº„Éâ„Çí1„Å§„Å†„ÅëÈÅ∏„Å≥„ÄÅÂçäËßíÊï∞Â≠ó„ÅßÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
        "ÂΩì„Å¶„ÅØ„Åæ„Çâ„Å™„Åë„Çå„Å∞Á©∫Ê¨Ñ„ÅßËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n"
        f"„Çø„Ç§„Éà„É´: {title}\nË¶ÅÁ¥Ñ: {summary}"
    )
    return _ask(choice_prompt)

def build_card(meta: dict, url: str, access_date: str) -> str:
    """Return full markdown string for one card."""
    domain = urlparse(url).netloc
    # ----- NDC classification
    # ÔºàÈÄî‰∏≠„ÅßÂ§±Êïó„Åó„Å¶„ÇÇ str „ÇíËøî„ÅôÂøÖË¶Å„Åå„ÅÇ„Çã„Åü„ÇÅ„ÄÅ
    #  „Åì„Åì„Åã„Çâ return „Åô„Çã„ÅÆ„ÅØÂé≥Á¶ÅÔºâ

    source_lang = detect_lang(meta['text'][:1000])
    needs_translation = source_lang != "ja"

    # summary: always Japanese, regardless of source language
    summary = ask_openai(
        f"Ê¨°„ÅÆÊñáÁ´†„ÇíÊó•Êú¨Ë™û„Åß300Â≠óÁ®ãÂ∫¶„ÅßË¶ÅÁ¥Ñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n{meta['text']}",
        SUMMARY_TOK,
    )
    summary = " ".join(summary.split())
    # store summary for later digest
    meta["summary"] = summary

    ndc_stub = classify_ndc_llm(meta["title"], summary)
    # store for downstream use
    meta["ndc"] = ndc_stub

    # automatic keywords from summary using LLM
    auto_keywords = extract_keywords_llm(summary)
    keywords_combined = list(dict.fromkeys((meta["keywords"] or []) + auto_keywords))

    original_text = tidy_markdown_para(meta['text'])
    translation = (
        tidy_markdown_para(translate_full(meta['text']))
        if needs_translation else ""
    )

    front = {
        "title": meta["title"],
        "url": url,
        "publication_date": meta["publication_date"],
        "access_date": access_date,
        "author": [{"family": meta["author_family"], "given": meta["author_given"]}]
                  if meta["author_family"] else [],
        "domain": domain,
        "ndc": ndc_stub,
        "keywords": keywords_combined,
        "summary": summary,
        "has_translation": needs_translation,
    }
    front_matter = yaml.safe_dump(
        front,
        allow_unicode=True,
        sort_keys=False,
        width=4096          # avoid PyYAML auto‚Äëwrapping
    ).strip()
    parts = ["---", front_matter, "---", ""]

    if needs_translation:
        parts += ["## Translation ÔºàÂíåË®≥Ôºâ", "", translation, ""]

    parts += ["## Original Text", "", original_text]

    # --- ALWAYS return str ---
    body = "\n".join(parts).lstrip()
    return body

def save_card(content: str, meta: dict) -> pathlib.Path:
    """
    Save markdown content to ./Library/ using
    {publication_date}-{slug}.md  where slug preserves Japanese/
    multibyte characters.

    If the title is entirely non‚ÄëASCII and slugify would Latin‚Äë
    transliterate it into unreadable romaji, we instead keep the
    original Unicode (with symbols sanitized) so the filename is still
    human‚Äëlegible.  Length is capped to 40 chars to avoid extremely
    long path names.
    """
    ndc_code = meta.get("ndc") or "_uncategorized"

    if ndc_code != "_uncategorized":
        # attach an English‚Äëstyle slug of the Japanese or English label for readability
        labels      = NDC_LABELS.get(ndc_code, {"ja": "", "en": ""})
        label_en_src = labels.get("en") or labels.get("ja")
        label_en     = slugify(label_en_src, allow_unicode=False) or "misc"
        subdir       = LIBRARY_DIR / f"{ndc_code}_{label_en}"
    else:   
        subdir = LIBRARY_DIR / ndc_code

    subdir.mkdir(parents=True, exist_ok=True)

    date_part = meta["publication_date"] or datetime.date.today().isoformat()
    slug_part = slugify(meta["title"], allow_unicode=True)[:40] or "untitled"
    path = subdir / f"{date_part}-{slug_part}.md"

    path.write_text(content, encoding="utf-8")
    return path

# ---------- main -----------------------------------------------------------

# ---------- cli / entry‚Äëpoint ---------------------------------------------

def cli():
    parser = argparse.ArgumentParser(
        description="Generate YAML‚ÄëFront‚ÄëMatter + Markdown information cards from URLs."
    )
    parser.add_argument("url_file", nargs="?", help="Path to txt file containing URLs (one per line)")
    parser.add_argument("--key", help="OpenAI API key (overrides env var)")
    parser.add_argument("-t", "--test", action="store_true",
                        help="Only test the supplied / detected API key and exit")
    args = parser.parse_args()

    # resolve API key
    openai.api_key = args.key or os.getenv("OPENAI_API_KEY") \
                     or getpass.getpass("Enter your OpenAI API key: ").strip()

    if args.test:
        try:
            pong = ask_openai("Say 'pong' in one word.", max_tokens=5)
            print(f"‚úÖ API key works. LLM responded: {pong}")
        except Exception as e:
            print(f"‚ùå API test failed: {e}")
        return

    if not args.url_file:
        parser.error("url_file is required unless --test is supplied.")

    generate_from_file(args.url_file)

# ---------- orchestrator ---------------------------------------------------

def generate_from_file(url_file: str):
    access_date = datetime.date.today().isoformat()
    with open(url_file, encoding="utf-8") as f:
        urls = [u.strip() for u in f if u.strip()]

    new_entries = []   # collect (title, pub_date, ndc, summary, rel_path)
    error_entries = [] # collect (url, error_str)
    for url in tqdm(urls, desc="Processing"):
        try:
            html  = fetch_html(url)
            meta  = extract_meta(url, html)
            card  = build_card(meta, url, access_date)
            fp    = save_card(card, meta)
            rel = fp.relative_to(LIBRARY_DIR)
            new_entries.append((
                meta["title"],
                meta["publication_date"],
                meta.get("ndc", ""),
                meta["summary"],
                rel
            ))
            tqdm.write(f"‚úì {fp}")
        except Exception as e:
            tqdm.write(f"‚ö†Ô∏è  {url}: {e}")
            error_entries.append((url, str(e)))

    # -------- write daily digest -----------
    if new_entries:
        today = datetime.date.today().isoformat()
        def _dt(d):
            try:
                return dtparser.parse(d)
            except Exception:
                return datetime.datetime(1970, 1, 1)
        # enumerate to keep original order for tie‚Äëbreak
        sorted_entries = sorted(
            enumerate(new_entries),
            key=lambda t: (
                -_dt(t[1][1]).timestamp(),          # publication date desc
                t[1][2],                            # ndc asc
                t[0]                                # original order
            )
        )
        lines = [f"# New Cards created on {today}", ""]
        for _, (title, pubdate, ndc, summ, rel) in sorted_entries:
            lines += [
                f"### [{title}]({rel})",
                f"- Publication date: {pubdate or '‚Äï'}",
                "",
                tidy_markdown_para(summ),
                ""
            ]
        if error_entries:
            lines += ["---", "## Error log", ""]
            for url, err in error_entries:
                lines.append(f"- **{url}**: {err}")
        DIGEST_FILE.write_text("\n".join(lines), encoding="utf-8")
        print(f"üìù Digest written to {DIGEST_FILE}")

if __name__ == "__main__":
    cli()